{
    "contents" : "---\noutput: pdf_document\nbibliography: r4bd.bib\n---\n\n```{r echo=FALSE}\nlibrary(\"grid\")\nlibrary(\"png\")\nlibrary(\"Rcpp\")\nlibrary(\"pryr\")\nlibrary(\"bigvis\")\n```\n\n# Introduction\n\n## What is Big Data?\n\nA common definition of big data is data that\nis:^[Data here is treated as a mass noun, similar to information.\nPurists may insist that 'data' should always plural because it originates from the Latin word datum.\nHowever, language evolves, we no longer speak Latin and the singular is becoming the norm [@kitchin2014data].]\n\n- Variable, within each dataset and between sources\n- Voluminous, occupying much RAM and hard disk space\n- High in velocity: it's always being generated\n\n\\noindent Precisely how variable, voluminous and rapidly generated data needs to be before it's classified as 'big' is rarely specified, however.\nLooser definitions recognise that Big Data is an umbrella or 'catch all' term, used to refer to information that is simply tricky to analyse using established methods [@Lovelace2015].\nWe use this looser definition in this in this book.\n\nThe variety of new datasets becoming available\nis huge. Therefore, instead of trying to cover all manner of new datasets,\nthe focus of this book is developing a solid understanding of the R *language* to interact with data.\nAs with learning any new language, a deep understanding of the fundamentals will provide the flexibility to deal with almost any situation.\nLearning to avoid computational bottlenecks and write efficient code, for example, can save hours of processing and development time.\nIn other words, **becoming proficient in handling Big Data entails first becoming fluent in data analysis and computing more generally**.\n\nIt's easy to get side-tracked or 'lost in the data' when analysing large datasets.\nClearly defining the aim of a particular analysis project therefore particularly important in this context.\nThere are often many ways to solve a problem with R and,\nin addition to computational speed, the most appropriate solution will likely depend on:\n\n- ease and speed of writing the code;\n- ease of communicating and reproducing the analysis;\n- durability of code.\n\n```{r drill, fig.margin=TRUE, fig.cap= \"A drill is analogous to a software tool: the questions of functionality and reliability should trump the question: 'is it the best?'\", echo=FALSE}\ngrid.raster(readPNG(\"figures//746px-Pistol-grip_drill.svg.png\"))\n```\n\nIn this context it is useful to think of software as a power-tool (Fig. 1.1). People rarely ask 'is this the BEST possible drill'?.\nMore likely a good builder will ask: 'is this drill *good enough* to get the job done?' 'is this drill robust?' and 'will it work in 20 years time?' The same applies to R for Big Data.\n\nRegardless of the 'big' dataset you hope to use,\nyou can be confident of one thing:\n**it is unlikely to be ready to analyse.**\nThis means that you must work to tidy the data,\na task that typically takes around\n80% of the effort expended on data analysis projects\n[@tidy-data]. \n\n## Coping with big data in R\n\nR has had a difficult relationship with big data. One of R's key features is that it loads data into the computer's\nRAM^[There are, however, packages such as **dplyr** which allow R to access, filter and even process data stored remotely.\nThese are described in chapter 5.].\nThis was less of a problem twenty years ago, when data sets were small and the main bottleneck on analysis was how quickly a statistician could think. Traditionally, the development of a statistical model took more time than the computation. When it comes to Big Data, this changes.\nNowadays datasets that are larger than your laptop's memory are commonplace.\n\nEven if the original data set is relatively small data set, the analysis can generate large objects. For example, suppose we went to perform standard cluster analysis. Using the built-in data set `USAarrests`, we can calculate a distance matrix,\n```{r}\nd = dist(USArrests)\n```\n\n\\noindent and perform hierarchical clustering to get a dendrogram\n\n```{r}\nfit = hclust(d)\n```\n\n\\noindent to get a dendrogram\n\n```{r denofig.fullwidth=TRUE, fig.height=2, echo=2, fig.cap=\"Dendrogram from USArrests data.\"}\npar(mar=c(3,3,2,1), mgp=c(2,0.4,0), tck=-.01,cex=0.5, las=1)\nplot(fit, labels=rownames(d))\n```\n\n\\noindent When we inspect the object size of the original data set and the distance object\n```{r}\npryr::object_size(USArrests)\npryr::object_size(d)\n```\n\n\\noindent we have managed to create an object that is three times larger than the original data set.^[The function \\texttt{object\\_size} is part of the \\texttt{pryr} package, which we will cover in chapter 2.] In fact the object `d` is a symmetric $n \\times n$ matrix, where $n$ is the number of rows in `USAarrests`. Clearly, as `n` increases the size of `d` increases at rate $O(n^2)$. So if our original data set contained $10,000$ records, the associated distance matrix would contain almost $10^8$ values. Of course since the matrix is symmetric, this corresponds to around $50$ million unique values.\n\nTo tackle big data in R, we review some of the possible strategies available.\n\n### Buy more RAM\n\nSince R keeps all objects in memory, the easiest way to deal with memory issues. Currently, 16GB costs less than £100. This small cost is quickly recuperated on user time. A relatively powerful desktop machine can be purchased for less that £1000. \n\nAnother alternative, could be to use cloud computing. For example, Amazon currently charge around 3£0.15 per Gigabyte of RAM. Currently, a $244$GB machine, with 32 cores, costs around £3.12 per hour (see [aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/).\n\n### Sampling\n\nDo you **really** need to load all of data at once? For example, if your data contains information regarding sales, does it make sense to aggregate across countries, or should the data be split up? Assuming that you need to analyse all of your data, then random sampling could provide an easy way to perform your analysis. In fact, it is almost always sensible to sample your data set at the beginning of an analysis until your analysis pipeline is in reasonable shape.\n\nIf your dataset is too large to read into RAM, it may need to be\n*preprocessed* or *filtered* using tools external to R before\nreading it in. This is the topic of chapter 3.\nFor databases we can filter when asking for the data from within R\n(described in chapter 6).\n\n### Integration with C++ or Java\n\nAnother strategy to improve performance, is to move small parts of the program from R to another, faster language, such as C++ or Java. The goal is to keep R's neat way of handling data, with the higher performance offered by other languages. Indeed, many of R's base functions are written in C or Fortran. This outsourcing of code to another language can be easily hidden in another function. \n\n### Avoid storing objects in memory\n\nThere are packages available that avoid storing data in memory. Instead objects are stored on your hard disc and analysed in blocks or chunks. Hadoop is an example of this technique. This strategy is perfect for dealing with large amounts of data. Unfortunately, many algorithms haven't been designed with this principle in mind. This means that only a few R functions that have been explicitly created to deal with specific chunk data types will work.\n\nThe two most famous packages on CRAN that use this principle are `ff` and `ffbase`.^[There is also the `bigmemory` package that does something similar.] The commercial product,  Revolution R Enterprise, also uses the chunk strategy in their `scaleR` package.\n\n### Alternative interpreters\n\nDue to the popularity of R, it now possible to use alternative interpreters (the interpreter is where the code is run). There are currently four possibilities\n\n * [pqrR](http://www.pqr-project.org/) (pretty quick R) is a new version of the R interpreter. One major downside, is that it is based on R-2.15.0. The developer (Radford Neal) has made many improvements, some of which have now been incorporated into base R. **pqR** is an open-source project licensed under the GPL. One notable improvement in pqR is that it is able to do some numeric computations in parallel with each other, and with other operations of the interpreter, on systems with multiple processors or processor cores.\n \n  * [Renjin](http://www.renjin.org/) reimplements the R interpreter in Java, so it can run on the Java Virtual Machine (JVM). Since R will be pure Java, it can run anywhere.\n\n  * [Tibco](http://spotfire.tibco.com/) created a C++ based interpreter called TERR. \n\n  * Oracle also offer an R-interpreter, that uses  Intel's mathematics library and therefore achieves a higher performance without changing R's core. \n\n## Course R package\n\nThere is companion R package for this course. The package contains some example data sets, and also a few helper functions. To install the package, first install `drat`.^[The `drat` package provides a nice way of accessing other package repositories.]\n\n```{r eval=FALSE}\ninstall.packages(\"drat\")\n```\n\n\\noindent Then the course package can be installed using^[Assuming you are using at least R version 3.2.0.]\n\n```{r eval=FALSE}\ndrat::addRepo(\"rcourses\")\ninstall.packages(\"r4bd\")\n```\n\n\n\n\n",
    "created" : 1467025353301.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4000525988",
    "id" : "3F98BE7B",
    "lastKnownWriteTime" : 1467025628,
    "path" : "F:/backup from laptop/data/data for geospatial python/R-for-Big-Data-master/01-Introduction.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}